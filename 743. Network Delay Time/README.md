# 743. 网络延迟时间

有 `N` 个网络节点，标记为 `1` 到 `N`。

给定一个列表 `times`，表示信号经过 **有向** 边的传递时间。 `times[i] = (u, v, w)`，其中 `u` 是源节点，`v` 是目标节点， `w` 是一个信号从源节点传递到目标节点的时间。

现在，我们向当前的节点 `K` 发送了一个信号。需要多久才能使 **所有节点**（共 N 个节点）都收到信号？如果不能使所有节点收到信号，返回 `-1`。

**注意：**

1. `N` 的范围在 `[1, 100]` 之间。
2. `K` 的范围在 `[1, N]` 之间。
3. `times` 的长度在 `[1, 6000]` 之间。
4. 所有的边 `times[i] = (u, v, w)` 都有 `1 <= u, v <= N` 且 `1 <= w <= 100`。

# 思路

1. 首先，这是一道有权图求最短路径的问题，且边的权重不存在负数，所以选择使用 **狄克斯特拉算法（dijkstra）**。
2. 由于狄克斯特拉算法中，我们总是需要从 **剩余节点** 中取出一个 **距离起点最近** 的节点出来，并更新它能到达的节点的距离。所以我们选择使用一个优先队列保存剩余节点，可以节约我们获取距离起点距离最近节点的耗时。
3. 最后，我们取出所有节点中距离起点最远的一个节点的距离，即耗时最长的节点。
4. 如果这个值是 Integer.MAX_VALUE，则说明无法到达这个节点，返回 -1。

# 实现

[Solution.java](https://github.com/afei-cn/LeetCode/blob/master/743.%20Network%20Delay%20Time/src/Solution.java)